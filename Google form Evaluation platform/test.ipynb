{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\TechM\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "e:\\TechM\\.venv\\Lib\\site-packages\\langchain\\chains\\api\\base.py:56: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from langchain_community.utilities.requests import TextRequestsWrapper\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import getpass\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from langchain_google_genai.chat_models import ChatGoogleGenerativeAI\n",
    "from langchain.agents import initialize_agent, Tool, AgentType\n",
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring the gemini api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_API_KEY\"] = \"Your gemini key here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"GOOGLE_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = getpass(\"Provide your Google API key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=ChatGoogleGenerativeAI(model='gemini-1.5-flash')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logic for evaluation tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function extracts questions and marks assigned to student from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reader(row):\n",
    "    query = (\n",
    "        f\"Your task is to extract the questions along with the marks assigned to that question and VERY IMPORTANT (full answers) of students present in the string. {row}\",\n",
    "        \"You have to extract full answers. VERY IMPORTANT: DON'T PRINT ANYTHING EXCEPT EXTRACTED CONTENT.\"\n",
    "    )\n",
    "    result = llm.invoke(query) \n",
    "    return result.content  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function evaluates the answers and gives marks to each student fo reach question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluator(input_str):\n",
    "    query = (\n",
    "        f\"Your task is to evaluate answers and give marks as output along with question number. {input_str}. Give marks accordingly, not just 10 or 0 and make sure to give fair marks for all students (VERY IMPORTANT).\",\n",
    "        \"Just give output as marks of each question. In a csv format where the question is first column and score is second\",\n",
    "       \n",
    "    )\n",
    "    result = llm.invoke(query)  \n",
    "    return result.content "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function provides insights to each student on each question why the marks got deducted and if the student scored full marks it just returns well done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insights(input_str):\n",
    "    query=(\n",
    "        f\"Your task is to provide insights on the answers and if the answer is correct just return well done {input_str}\",\n",
    "        \"Just give output as insights of each question and nothing else in insight column. In a csv format where the question number is first column and insights is second(should contain only one line)\",\n",
    "        \"VERY IMPORTANT (the provided insight must be short)\"\n",
    "    )\n",
    "    result=llm.invoke(query)\n",
    "    return result.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This fucntion is used to format the text generated by the llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_symbols(text):\n",
    "    cleaned_text = re.sub(r'[^\\w\\s]', '/', text)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function pass the data to the above functions row wise from the uploaded excel file and then convert the data into a dataframe which has 2 columns one is question number and other is score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_marks=None\n",
    "def iterator1(path):\n",
    "    global df_final_marks\n",
    "    all_records = []\n",
    "    df = pd.read_csv(path)\n",
    "    for index, row in df.iterrows():\n",
    "        extracted_content = reader(row)  \n",
    "        evaluated_marks = evaluator(extracted_content)  \n",
    "        cleaned_text = remove_symbols(evaluated_marks)  \n",
    "        lines = cleaned_text.strip().split('\\n')[1:]  \n",
    "        records = [line.split('/') for line in lines]\n",
    "        all_records.extend(records)\n",
    "    df_final_marks = pd.DataFrame(all_records, columns=['Question', 'Score'])\n",
    "    df_final_marks['Score'] = pd.to_numeric(df_final_marks['Score'])\n",
    "    df_final_marks=df_final_marks\n",
    "    print(df_final_marks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function pass the data to the above functions row wise from the uploaded excel file and then convert the data into a dataframe which has 2 columns one is question number and other is insisghts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterator2(path):\n",
    "    df=pd.read_csv(path)\n",
    "    all_records1=[]\n",
    "    global df_final_insights\n",
    "    for index, row in df.iterrows():\n",
    "        extracted_content = reader(row)  \n",
    "        evaluated_marks = insights(extracted_content)  \n",
    "        cleaned_text = remove_symbols(evaluated_marks) \n",
    "        lines=cleaned_text.strip().split('\\n')[1:]\n",
    "        records=[line.split('/')[:2] for line in lines]\n",
    "        all_records1.extend(records)\n",
    "    df_final_insights=pd.DataFrame(all_records1,columns=['Question','Insights'])\n",
    "    print(df_final_insights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function passes the path of the file to both evaluation and insight function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readcsv(path):\n",
    "    iterator1(path)\n",
    "    iterator2(path)\n",
    "    return path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is to merge and format and store it to a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_database(path):\n",
    "    global final_df_data\n",
    "    global df_final_insights\n",
    "    df_final_insights=df_final_insights.drop('Question',axis=1)\n",
    "    df_data=pd.concat([df_final_marks,df_final_insights],axis=1)\n",
    "    df=pd.read_csv(path)\n",
    "    username_column = df['Username']\n",
    "    score_columns = df.filter(like='[Score]')\n",
    "    df_extracted = pd.concat([username_column, score_columns], axis=1)\n",
    "    df_extracted.loc[:, df_extracted.columns != 'Username'] = df_extracted.loc[:, df_extracted.columns != 'username'].applymap(lambda x: ''.join(filter(str.isdigit, str(x))))\n",
    "    total_rows = len(df_extracted)\n",
    "    df=df_extracted.merge(df_data,how='cross')\n",
    "    m = total_rows\n",
    "    first_user_email = df['Username'].iloc[0]\n",
    "    first_user_data = df[df['Username'] == first_user_email].head(m)\n",
    "    other_users_data = df[df['Username'] != first_user_email].groupby('Username').tail(m)\n",
    "    final_df_data = pd.concat([first_user_data, other_users_data])\n",
    "    final_df_data.reset_index(drop=True, inplace=True)\n",
    "    print(final_df_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining tools for langchain tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_tool = Tool(\n",
    "    name='Response Evaluation Tool',\n",
    "    func=readcsv,\n",
    "    description='The task is to pass the path to the readcsv function'\n",
    ")\n",
    "database_tool = Tool(\n",
    "    name='Database Tool',\n",
    "    func=create_database,  \n",
    "    description='The task is to pass the path to the create_database function'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [evaluator_tool,database_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ishaa\\AppData\\Local\\Temp\\ipykernel_30316\\3940763615.py:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferWindowMemory(\n"
     ]
    }
   ],
   "source": [
    "memory = ConversationBufferWindowMemory(\n",
    "    memory_key='chat_history',\n",
    "    k=3,\n",
    "    return_messages=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ishaa\\AppData\\Local\\Temp\\ipykernel_30316\\2219053028.py:1: LangChainDeprecationWarning: The function `initialize_agent` was deprecated in LangChain 0.1.0 and will be removed in 1.0. Use :meth:`~Use new agent constructor methods like create_react_agent, create_json_agent, create_structured_chat_agent, etc.` instead.\n",
      "  conversational_agent_evaluator=initialize_agent(\n"
     ]
    }
   ],
   "source": [
    "conversational_agent_evaluator=initialize_agent(\n",
    "    agent='chat-conversational-react-description',\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    max_iterations=3,\n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logic for custom agent run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Question  Score\n",
      "0  Question 1     10\n",
      "1  Question 2      0\n",
      "2  Question 1     10\n",
      "3  Question 2     10\n",
      "  Question                                  Insights\n",
      "0        1                                Well done \n",
      "1        2  The answer does not address the question\n",
      "2        1                                 Well done\n",
      "3        2                                 Well done\n",
      "                      Username What is the SI unit of force? [Score]  \\\n",
      "0  malhotraishaan857@gmail.com                                    10   \n",
      "1  malhotraishaan857@gmail.com                                    10   \n",
      "2         malhotra90@gmail.com                                    10   \n",
      "3         malhotra90@gmail.com                                    10   \n",
      "\n",
      "  Explain the difference between mass and weight? [Score]    Question  Score  \\\n",
      "0                                                 10       Question 1     10   \n",
      "1                                                 10       Question 2      0   \n",
      "2                                                 10       Question 1     10   \n",
      "3                                                 10       Question 2     10   \n",
      "\n",
      "                                   Insights  \n",
      "0                                Well done   \n",
      "1  The answer does not address the question  \n",
      "2                                 Well done  \n",
      "3                                 Well done  \n",
      "Final output: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ishaa\\AppData\\Local\\Temp\\ipykernel_30316\\2887323470.py:10: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_extracted.loc[:, df_extracted.columns != 'Username'] = df_extracted.loc[:, df_extracted.columns != 'username'].applymap(lambda x: ''.join(filter(str.isdigit, str(x))))\n"
     ]
    }
   ],
   "source": [
    "def custom_agent_run(query):\n",
    "    result1 = evaluator_tool.run(query)  \n",
    "    result2 = database_tool.run(result1)  \n",
    "    return result2  \n",
    "path=input()\n",
    "output = custom_agent_run(path)\n",
    "print(f\"Final output: {output}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For students to view there marks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total marks scored are 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Username</th>\n",
       "      <th>What is the SI unit of force? [Score]</th>\n",
       "      <th>Explain the difference between mass and weight? [Score]</th>\n",
       "      <th>Question</th>\n",
       "      <th>Score</th>\n",
       "      <th>Insights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>malhotraishaan857@gmail.com</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>Question 1</td>\n",
       "      <td>10</td>\n",
       "      <td>Well done</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>malhotraishaan857@gmail.com</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>Question 2</td>\n",
       "      <td>0</td>\n",
       "      <td>The answer does not address the question</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Username What is the SI unit of force? [Score]  \\\n",
       "0  malhotraishaan857@gmail.com                                    10   \n",
       "1  malhotraishaan857@gmail.com                                    10   \n",
       "\n",
       "  Explain the difference between mass and weight? [Score]    Question  Score  \\\n",
       "0                                                 10       Question 1     10   \n",
       "1                                                 10       Question 2      0   \n",
       "\n",
       "                                   Insights  \n",
       "0                                Well done   \n",
       "1  The answer does not address the question  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_username = 'malhotraishaan857@gmail.com'\n",
    "filtered_df = final_df_data[final_df_data['Username'] == input_username]\n",
    "total_marks=filtered_df['Score'].sum()\n",
    "print(f\"The total marks scored are {total_marks}\")\n",
    "filtered_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
